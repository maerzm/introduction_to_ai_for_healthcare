<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:"Aptos Display";}
@font-face
	{font-family:Aptos;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:8.0pt;
	margin-left:0cm;
	line-height:115%;
	font-size:12.0pt;
	font-family:"Aptos",sans-serif;}
h1
	{mso-style-link:"Überschrift 1 Zchn";
	margin-top:18.0pt;
	margin-right:0cm;
	margin-bottom:4.0pt;
	margin-left:0cm;
	page-break-after:avoid;
	font-size:20.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h2
	{mso-style-link:"Überschrift 2 Zchn";
	margin-top:8.0pt;
	margin-right:0cm;
	margin-bottom:4.0pt;
	margin-left:0cm;
	page-break-after:avoid;
	font-size:16.0pt;
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;
	font-weight:normal;}
h3
	{mso-style-link:"Überschrift 3 Zchn";
	margin-top:8.0pt;
	margin-right:0cm;
	margin-bottom:4.0pt;
	margin-left:0cm;
	page-break-after:avoid;
	font-size:14.0pt;
	font-family:"Aptos",sans-serif;
	color:#0F4761;
	font-weight:normal;}
a:link, span.MsoHyperlink
	{color:#467886;
	text-decoration:underline;}
p
	{margin:0cm;
	font-size:12.0pt;
	font-family:"Times New Roman",serif;}
span.berschrift1Zchn
	{mso-style-name:"Überschrift 1 Zchn";
	mso-style-link:"Überschrift 1";
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;}
span.berschrift2Zchn
	{mso-style-name:"Überschrift 2 Zchn";
	mso-style-link:"Überschrift 2";
	font-family:"Aptos Display",sans-serif;
	color:#0F4761;}
span.berschrift3Zchn
	{mso-style-name:"Überschrift 3 Zchn";
	mso-style-link:"Überschrift 3";
	font-family:"Times New Roman",serif;
	color:#0F4761;}
.MsoChpDefault
	{font-size:12.0pt;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:115%;}
@page WordSection1
	{size:595.3pt 841.9pt;
	margin:70.85pt 70.85pt 2.0cm 70.85pt;}
div.WordSection1
	{page:WordSection1;}
@page WordSection2
	{size:595.3pt 841.9pt;
	margin:70.85pt 70.85pt 2.0cm 70.85pt;}
div.WordSection2
	{page:WordSection2;}
 /* List Definitions */
 ol
	{margin-bottom:0cm;}
ul
	{margin-bottom:0cm;}
-->
</style>

</head>

<body lang=DE link="#467886" vlink="#96607D" style='word-wrap:break-word'>

<div class=WordSection1>

<h1><img width=265 height=265 src="Chapter%204-Dateien/image001.jpg"
align=left hspace=12><a name="_Toc177298514"></a><a name="_Toc179448984"></a><a
name="_Chapter_4:_Ethical"></a><span lang=EN-US>Chapter 4: Ethical and Legal
Considerations</span></h1>

<p class=MsoNormal><span lang=EN-US style='font-family:"Calibri",sans-serif'>As
AI continues to advance and integrate into healthcare, it is crucial to address
the ethical and legal considerations to ensure responsible and fair use of
these technologies.</span></p>

<p class=MsoNormal style='line-height:107%'><span lang=EN-US style='font-family:
"Calibri",sans-serif'>Ethical considerations are essential in ensuring the
responsible use of AI in healthcare. Addressing these ethical issues helps
protect patients' rights and ensures that AI systems are used in a way that is
fair, beneficial, and safe.</span></p>

<p class=MsoNormal style='line-height:107%'><b><span lang=EN-US
style='font-family:"Calibri",sans-serif'>&nbsp;</span></b></p>

<p class=MsoNormal style='line-height:107%'><b><span lang=EN-US
style='font-family:"Calibri",sans-serif'>&nbsp;</span></b></p>

<p class=MsoNormal style='line-height:107%'><b><span lang=EN-US
style='font-family:"Calibri",sans-serif'>&nbsp;</span></b></p>

<p class=MsoNormal style='line-height:107%'><b><span lang=EN-US
style='font-family:"Calibri",sans-serif'>Transparency:</span></b><span
lang=EN-US style='font-family:"Calibri",sans-serif'> AI systems should be
transparent and explainable, meaning that patients and healthcare providers
should understand how decisions are made by the AI. This helps build trust and
ensures that AI recommendations or decisions can be questioned and verified when
necessary.</span></p>

<p class=MsoNormal style='line-height:107%'><b><span lang=EN-US
style='font-family:"Calibri",sans-serif'>Accountability:</span></b><span
lang=EN-US style='font-family:"Calibri",sans-serif'> Clear lines of
accountability must be established for AI systems. This means identifying who
is responsible for the outcomes of AI applications—whether it’s the developers,
healthcare providers, or the institutions deploying the AI. Accountability
ensures that someone is always responsible for addressing any issues or errors
that arise.</span></p>

<p class=MsoNormal style='line-height:107%'><b><span lang=EN-US
style='font-family:"Calibri",sans-serif'>Beneficence:</span></b><span
lang=EN-US style='font-family:"Calibri",sans-serif'> AI systems should be
designed and used with the goal of benefiting patients. This means using AI to
improve healthcare outcomes, enhance the quality of care, and ensure that
patient well-being is the primary focus of AI applications.</span></p>

<p class=MsoNormal style='line-height:107%'><b><span lang=EN-US
style='font-family:"Calibri",sans-serif'>Non-Maleficence:</span></b><span
lang=EN-US style='font-family:"Calibri",sans-serif'> AI systems must be
designed to avoid causing harm to patients. This includes preventing unintended
negative consequences, minimizing risks, and ensuring that AI use in healthcare
does not lead to harmful outcomes.</span></p>

<p class=MsoNormal><span lang=EN-US style='font-family:"Calibri",sans-serif'>Let’s
explore some real-world examples to make the topic of ethical AI more
relatable. These examples, though not necessarily from the healthcare field,
will help us understand the broader implications of ethical considerations in
AI. </span></p>

<h2><a name="_Toc177298515"></a><a name="_Toc179448985"><span lang=EN-US
style='font-family:"Calibri",sans-serif'>Data Privacy</span></a><span
lang=EN-US style='font-family:"Calibri",sans-serif'> </span></h2>

<h3><a name="_Toc179448986"><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>Example: The Cambridge Analytica – Facebook scandal</span></a></h3>

<p class=MsoNormal><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>The Cambridge Analytica scandal is a stark reminder of the power
and potential misuse of personal data in the digital age. This case not only highlights
the ethical implications of data privacy but also underscores the importance of
trust and compliance in any field that handles sensitive information. For
healthcare students, understanding this scandal can provide valuable insights
into the ethical considerations and responsibilities that come with managing
personal data.</span></p>

<p style='margin-bottom:8.0pt;line-height:107%'><b><span lang=EN-US
style='font-size:11.0pt;line-height:107%;font-family:"Calibri",sans-serif;
color:#156082'>The Scandal Unfolds</span></b><span lang=EN-US style='font-size:
11.0pt;line-height:107%;font-family:"Calibri",sans-serif;color:#156082'>&nbsp;In
the mid-2010s, a British consulting firm called Cambridge Analytica collected
personal data from millions of Facebook users without their consent. This data
was harvested through an app called “This Is Your Digital Life,” which was
developed by a data scientist named Aleksandr Kogan. The app asked users to
answer some questions to build psychological profiles, but it also collected
data from their Facebook friends, affecting up to 87 million profiles.</span></p>

<p style='margin-bottom:8.0pt;line-height:107%'><b><span lang=EN-US
style='font-size:11.0pt;line-height:107%;font-family:"Calibri",sans-serif;
color:#156082'>The Impact</span></b><span lang=EN-US style='font-size:11.0pt;
line-height:107%;font-family:"Calibri",sans-serif;color:#156082'>&nbsp;Cambridge
Analytica used this data to influence political campaigns, including the 2016
U.S. presidential election and the Brexit referendum. The scandal came to light
in 2018, thanks to whistleblower Christopher Wylie. This revelation led to a
massive public outcry and significant legal consequences for Facebook,
including a $5 billion fine by the Federal Trade Commission.</span></p>

<p style='margin-bottom:8.0pt;line-height:107%'><b><span lang=EN-US
style='font-size:11.0pt;line-height:107%;font-family:"Calibri",sans-serif;
color:#156082'>How Cambridge Analytica Used Other Campaigns</span></b><span
lang=EN-US style='font-size:11.0pt;line-height:107%;font-family:"Calibri",sans-serif;
color:#156082'>&nbsp;Cambridge Analytica was involved in various political
campaigns around the world, which helped them refine their techniques for
manipulating public opinion. For example, they worked on the 2015 Nigerian
presidential election, where they reportedly used data to target voters with
tailored messages designed to influence their opinions and behaviors. They also
worked on campaigns in Kenya, Malaysia, and India, using similar tactics to
sway voters.</span></p>

<p style='margin-bottom:8.0pt;line-height:107%'><span lang=EN-US
style='font-size:11.0pt;line-height:107%;font-family:"Calibri",sans-serif;
color:#156082'>Their approach often involved creating highly targeted
advertisements and content based on the psychological profiles they built from
the harvested data. By understanding the fears, desires, and motivations of
different demographic groups, they could craft messages that resonated deeply
with individuals, thereby increasing the effectiveness of their campaigns.
Importantly, these messages were not always truthful and often included
misinformation to manipulate public opinion.</span></p>

<p><span lang=EN-US style='font-size:11.0pt;font-family:"Calibri",sans-serif;
color:#156082'>Protecting patient privacy and ensuring the security of their
information is crucial in healthcare. AI systems that handle patient data must
follow strict data protection regulations, such as the General Data Protection
Regulation (GDPR) in Europe, to keep patient information safe and confidential.</span></p>

<h3><a name="_Toc179448987"><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>Healthcare</span></a><span lang=EN-US style='font-size:11.0pt;
font-family:"Calibri",sans-serif;color:#156082'> </span><span lang=EN-US
style='font-family:"Calibri",sans-serif;color:#156082'>Examples</span><span
lang=EN-US style='font-size:11.0pt;font-family:"Calibri",sans-serif;color:#156082'>
</span></h3>

<p><span lang=EN-US style='font-size:11.0pt;font-family:"Calibri",sans-serif;
color:#156082'>Besides the misuse of data, cyberattacks pose significant
threats to data privacy. Here are some real-world examples that highlight the
ethical implications of data privacy</span><span lang=EN-US style='font-size:
11.0pt;font-family:"Calibri",sans-serif;color:#156082'> in healthcare.</span></p>

<ol style='margin-top:0cm' start=1 type=1>
 <li class=MsoNormal style='color:#156082;line-height:107%'><span
     style='color:windowtext'><a
     href="https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2024.1454323/full"><b><span
     lang=EN-US style='font-family:"Calibri",sans-serif;color:#156082'>Google
     DeepMind and the Royal Free NHS Trust</span></b></a></span><span
     lang=EN-US style='font-family:"Calibri",sans-serif'>: In 2016, Google
     DeepMind partnered with the Royal Free NHS Trust in the UK to develop an
     app for detecting acute kidney injury. However, it was later revealed that
     the project had access to the personal health data of 1.6 million patients
     without their explicit consent.&nbsp;This raised serious concerns about
     patient privacy and the ethical use of health data.</span></li>
 <li class=MsoNormal style='color:#156082;line-height:107%'><span
     style='color:windowtext'><a
     href="https://theconversation.com/the-problems-that-occur-when-health-data-is-not-used-82453"><b><span
     lang=EN-US style='font-family:"Calibri",sans-serif;color:#156082'>Anthem
     Inc. Data Breach</span></b></a></span><span lang=EN-US style='font-family:
     "Calibri",sans-serif'>: In 2015, Anthem Inc., one of the largest health
     insurance companies in the US, suffered a massive data breach that exposed
     the personal information of nearly 80 million individuals.&nbsp;The breach
     included names, birthdates, social security numbers, and medical IDs,
     underscoring the importance of cybersecurity in protecting patient data.</span></li>
 <li class=MsoNormal style='color:#156082;line-height:107%'><span
     style='color:windowtext'><a
     href="https://theconversation.com/the-problems-that-occur-when-health-data-is-not-used-82453"><b><span
     lang=EN-US style='font-family:"Calibri",sans-serif;color:#156082'>UCLA
     Health System Cyberattack</span></b></a></span><span lang=EN-US
     style='font-family:"Calibri",sans-serif'>: In 2015, UCLA Health System
     experienced a cyberattack that compromised the personal and medical
     information of 4.5 million patients.&nbsp;The attackers gained access to
     names, addresses, dates of birth, social security numbers, and medical
     information, emphasizing the need for advanced security measures to
     protect against cyber threats.</span></li>
</ol>

<p class=MsoNormal><span lang=EN-US style='font-family:"Calibri",sans-serif'>&nbsp;</span></p>

<h3><a name="_Toc179448988"></a><a name="_Toc177298516"><span
class=berschrift2Zchn><span lang=EN-US style='font-size:16.0pt;font-family:
"Calibri",sans-serif'>Key Points</span></span></a><strong><span lang=EN-US
style='font-size:11.0pt;font-family:"Calibri",sans-serif'>:</span></strong></h3>

<p style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;margin-left:36.0pt;
text-indent:-17.85pt'><span lang=EN-US style='font-size:10.0pt;font-family:
Symbol'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><strong><span lang=EN-US style='font-size:11.0pt;font-family:
"Calibri",sans-serif'>Confidentiality:</span></strong><span lang=EN-US
style='font-size:11.0pt;font-family:"Calibri",sans-serif'> This means keeping
patient information private and making sure that only authorized individuals,
like healthcare professionals involved in the patient's care, can access it.
It's essential to prevent unauthorized access, whether by accident or through intentional
misuse.</span></p>

<p style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;margin-left:36.0pt;
text-indent:-17.85pt'><span style='font-size:10.0pt;font-family:Symbol'>·<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><strong><span lang=EN-US style='font-size:11.0pt;font-family:
"Calibri",sans-serif'>Security:</span></strong><span lang=EN-US
style='font-size:11.0pt;font-family:"Calibri",sans-serif'> To protect patient
data from being stolen or leaked, healthcare systems must use strong security
measures. </span><span style='font-size:11.0pt;font-family:"Calibri",sans-serif'>This
includes:</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>Encryption:</span></strong><span
lang=EN-US style='font-family:"Calibri",sans-serif'> Transforming data into a
code that can only be read by those who have the key.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>Secure
Access Controls:</span></strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>
Ensuring that only the right people have access to sensitive data, using
passwords, biometric checks, or other verification methods.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>Regular
Security Audits:</span></strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>
Checking the system regularly to identify and fix any vulnerabilities before
they can be exploited.</span></p>

<p style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;margin-left:36.0pt;
text-indent:-17.85pt'><span style='font-size:10.0pt;font-family:Symbol'>·<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><strong><span lang=EN-US style='font-size:11.0pt;font-family:
"Calibri",sans-serif'>Compliance:</span></strong><span lang=EN-US
style='font-size:11.0pt;font-family:"Calibri",sans-serif'> Following data
protection laws like GDPR is mandatory. </span><span style='font-size:11.0pt;
font-family:"Calibri",sans-serif'>This involves:</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>Getting
Consent:</span></strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>
Patients must give clear permission before their data is used, especially for
purposes beyond direct care.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>Data
Minimization:</span></strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>
Only the necessary amount of patient data should be collected and stored,
avoiding unnecessary exposure of personal information.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>Patient
Rights:</span></strong><span lang=EN-US style='font-family:"Calibri",sans-serif'>
Patients have the right to access their data, know how it is being used, and
request that their data be deleted if they choose.</span></p>

<p class=MsoNormal><span lang=EN-US style='font-family:"Calibri",sans-serif'>&nbsp;</span></p>

<h2><a name="_Toc175247479"></a><a name="_Toc177298517"></a><a
name="_Toc179448989"><span lang=EN-US style='font-family:"Calibri",sans-serif'>Bias
and Fairness</span></a></h2>

<p class=MsoNormal><span lang=EN-US style='font-family:"Calibri",sans-serif'>AI
systems in healthcare can unintentionally carry biases from their training
data, which might lead to unfair treatment of certain patient groups. It’s
crucial to identify and address these biases to ensure that everyone receives
fair and equitable healthcare. AI learns from the data it is trained on,
including any biases within that data. In this way, AI replicates the biases
present in its training data—if we have biases, the AI will exhibit those
biases as well.</span></p>

<h3><a name="_Toc179448990"><span lang=EN-US style='font-family:"Calibri",sans-serif'>Example:
The Amazon AI Recruiting Tool Scandal</span></a><span lang=EN-US
style='font-family:"Calibri",sans-serif'> </span></h3>

<p class=MsoNormal><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>The Amazon AI Recruiting Tool Scandal is a stark reminder of the
potential biases and ethical pitfalls in AI systems. This case not only
highlights the importance of fairness in AI but also underscores the need for
transparency and accountability in any field that utilizes automated
decision-making. For healthcare students, understanding this scandal can
provide valuable insights into the ethical considerations and responsibilities
that come with deploying AI technologies.</span></p>

<p class=MsoNormal><b><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>The Scandal Unfolds</span></b><span lang=EN-US style='font-family:
"Calibri",sans-serif;color:#156082'><br>
In the mid-2010s, Amazon developed an AI recruiting tool intended to streamline
the hiring process. However, it was discovered that the tool was biased against
women. The AI had been trained on resumes submitted to the company over a
10-year period, which were predominantly from men.&nbsp;As a result, the AI
system learned to favor male candidates and penalize resumes that included
terms associated with women.</span></p>

<p class=MsoNormal><b><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>The Impact</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'><br>
Amazon’s AI recruiting tool systematically discriminated against female
candidates, particularly for technical roles such as software engineering. The
tool downgraded resumes that included the word “women’s” (e.g., “women’s rugby
team”) and favored resumes with terms more commonly used by men.&nbsp;Despite
efforts to correct the bias, Amazon ultimately decided to discontinue the tool
in 2017</span><a
href="https://www.vice.com/en/article/amazon-ai-recruitment-hiring-tool-gender-bias/"
target="_blank"><sup><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>2</span></sup></a><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>.</span></p>

<p class=MsoNormal><b><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>How Amazon’s AI Tool Affected Other Hiring Practices</span></b><span
lang=EN-US style='font-family:"Calibri",sans-serif;color:#156082'><br>
The bias in Amazon’s AI recruiting tool is not an isolated incident but
reflects broader issues in AI and machine learning. Similar tools are used by
various companies worldwide, potentially perpetuating biases in hiring
practices. These tools often rely on historical data, which can embed existing
biases into the AI models.&nbsp;For example, AI systems trained on biased data
may inadvertently favor candidates from certain demographic groups while
disadvantaging others</span><a
href="https://www.forbes.com/councils/forbeshumanresourcescouncil/2021/10/14/understanding-bias-in-ai-enabled-hiring/"
target="_blank"><sup><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>3</span></sup></a><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>.</span></p>

<p class=MsoNormal><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>Their approach often involved analyzing resume text and other
application materials to identify patterns associated with successful
candidates. However, these patterns were based on historical hiring data, which
reflected the existing gender imbalance in the tech industry. As a result, the
AI tool reinforced these biases rather than eliminating them. Importantly,
these biases were not always apparent and often required careful scrutiny to
identify and address.</span></p>

<h3><a name="_Toc179448991"><span lang=EN-US style='font-family:"Calibri",sans-serif;
color:#156082'>Healthcare</span></a><span lang=EN-US style='font-size:11.0pt;
font-family:"Calibri",sans-serif;color:#156082'> </span><span lang=EN-US
style='font-family:"Calibri",sans-serif;color:#156082'>Examples</span><span
lang=EN-US style='font-size:11.0pt;font-family:"Calibri",sans-serif;color:#156082'>
</span></h3>

<p class=MsoNormal style='margin-top:9.0pt'><span lang=EN-US style='font-family:
"Calibri",sans-serif;color:#156082'>Here are some real-world examples that
highlight the ethical implications and biases in healthcare:</span></p>

<ol style='margin-top:0cm' start=1 type=1>
 <li class=MsoNormal style='color:#156082;margin-bottom:0cm;line-height:normal'><span
     style='color:windowtext'><a
     href="https://publichealth.jhu.edu/2024/pulse-oximeters-racial-bias"><b><span
     lang=EN-US style='font-family:"Calibri",sans-serif;color:#156082'>Racial
     Bias in Pulse Oximeters</span></b></a></span><span lang=EN-US
     style='font-family:"Calibri",sans-serif'>: Pulse oximeters, devices used
     to measure blood oxygen levels, have been found to be less accurate for
     patients with darker skin tones. Studies have shown that these devices can
     overestimate oxygen levels in Black and Hispanic patients, potentially
     leading to inadequate treatment for conditions like COVID-19.</span></li>
 <li class=MsoNormal style='color:#156082;margin-bottom:0cm;line-height:normal'><span
     style='color:windowtext'><a
     href="https://doi.org/10.1016/j.jopan.2022.09.004"><b><span lang=EN-US
     style='font-family:"Calibri",sans-serif;color:#156082'>Gender Bias in Pain
     Management</span></b></a></span><span lang=EN-US style='font-family:"Calibri",sans-serif'>:
     Research has shown that women are often undertreated for pain compared to
     men. A review of multiple studies found that women are more likely to have
     their pain dismissed as emotional or psychological, leading to delays in
     receiving appropriate pain management.</span></li>
 <li class=MsoNormal style='color:#156082;margin-bottom:0cm;line-height:normal'><span
     style='color:windowtext'><a
     href="https://doi.org/10.1007/s41666-023-00153-2"><b><span lang=EN-US
     style='font-family:"Calibri",sans-serif;color:#156082'>Bias in Electronic
     Health Records (EHRs)</span></b></a></span><span lang=EN-US
     style='font-family:"Calibri",sans-serif'>: EHRs can perpetuate biases
     present in the healthcare system. For example, if a physician’s notes
     reflect biased assumptions about a patient’s behavior or compliance, these
     biases can influence future care decisions. This can lead to disparities
     in treatment and outcomes for marginalized groups.</span></li>
 <li class=MsoNormal style='color:#156082;margin-bottom:0cm;line-height:normal'><span
     style='color:windowtext'><a
     href="http://dx.doi.org/10.1038/s41591-020-0942-0"><b><span lang=EN-US
     style='font-family:"Calibri",sans-serif;color:#156082'>AI in Diagnostic
     Tools</span></b></a></span><span lang=EN-US style='font-family:"Calibri",sans-serif'>:
     AI systems used for diagnosing diseases can inherit biases from the data
     they are trained on. For instance, an AI model trained predominantly on
     data from one demographic group may not perform as well for other groups,
     leading to misdiagnoses or unequal treatment. This has been a concern in
     areas like dermatology, where AI tools may not accurately diagnose skin
     conditions in patients with darker skin.</span></li>
</ol>

<h3><a name="_Toc179448992"><span lang=EN-US style='font-family:"Calibri",sans-serif'>Key
Points:</span></a></h3>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:36.0pt;text-indent:-17.85pt;line-height:normal'><span
style='font-size:10.0pt;font-family:Symbol'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Bias
Detection:</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>
It’s important to spot and reduce biases in AI algorithms. </span><span
style='font-family:"Calibri",sans-serif'>This can be done using methods like:</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Fairness-aware
Machine Learning:</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>
Techniques that adjust the AI to be more fair and equitable.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Bias
Audits:</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>
Regularly checking AI systems for biased outcomes.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Monitoring
AI Outputs:</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>
Continuously reviewing the AI's decisions to catch any unfair patterns.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:36.0pt;text-indent:-17.85pt;line-height:normal'><span
style='font-size:10.0pt;font-family:Symbol'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Fairness:</span></b><span
lang=EN-US style='font-family:"Calibri",sans-serif'> Ensuring that AI systems
are fair means making sure they don't favor or disadvantage any group of
patients. </span><span style='font-family:"Calibri",sans-serif'>Developers
should:</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Use
Fairness Metrics:</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>
These are standards that help measure whether the AI is treating all groups
fairly.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Follow
Guidelines:</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>
Specific rules and guidelines can be used to guide the development and
deployment of AI models to ensure fairness.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:36.0pt;text-indent:-17.85pt;line-height:normal'><span
style='font-size:10.0pt;font-family:Symbol'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Inclusivity:</span></b><span
lang=EN-US style='font-family:"Calibri",sans-serif'> To avoid bias, it’s vital
to use diverse and representative datasets when training AI models. </span><span
style='font-family:"Calibri",sans-serif'>This means:</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Diverse
Data</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>:
Ensuring that the training data includes a wide range of patient backgrounds
and conditions.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Avoiding
Underrepresentation</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>:
Making sure minority groups are well-represented in the data so that the AI can
serve everyone effectively.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:72.0pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Diverse
Teams</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>:
Teams developing AI models must be diverse in every sense, bringing together
different backgrounds, experiences, and perspectives. This natural diversity
helps identify blind spots, reduce bias, and create more effective and
inclusive AI solutions.</span></p>

<h2><a name="_Toc177298518"></a><a name="_Toc179448993"><span lang=EN-US
style='font-family:"Calibri",sans-serif'>The AI Act: Legal Framework and
Guidelines</span></a></h2>

<p class=MsoNormal><span lang=EN-US style='font-family:"Calibri",sans-serif'>Understanding
and adhering to legal frameworks and guidelines is critical for the safe and
ethical deployment of AI in healthcare. In Europe, the AI Act provides a
regulatory framework for AI systems, including generative AI and large language
models (LLMs). </span></p>

<p class=MsoNormal><span lang=EN-US style='font-family:"Calibri",sans-serif'>It
aims to foster responsible AI development and deployment in the EU. It
introduces a risk-based approach to AI regulation, categorizing AI systems into
minimal risk, specific transparency risk, high risk, and unacceptable risk.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:71.7pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Minimal
Risk</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>: AI
systems like spam filters face no obligations under the AI Act.</span></p>

<p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:6.0pt;
margin-left:71.7pt;text-indent:-17.85pt;line-height:normal'><span lang=EN-US
style='font-size:10.0pt;font-family:"Courier New"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;
</span></span><b><span lang=EN-US style='font-family:"Calibri",sans-serif'>Specific
Transparency Risk</span></b><span lang=EN-US style='font-family:"Calibri",sans-serif'>:
Systems like chatbots, including those powered by LLMs, must clearly inform
users that they are interacting with a machine.</span></p>

<ul style='margin-top:0cm' type=disc>
 <ul style='margin-top:0cm' type=circle>
  <li class=MsoNormal style='margin-bottom:6.0pt;line-height:normal'><b><span
      lang=EN-US style='font-family:"Calibri",sans-serif'>High Risk</span></b><span
      lang=EN-US style='font-family:"Calibri",sans-serif'>: AI systems used in
      healthcare, such as AI-based medical software and generative AI models
      for medical imaging, must comply with strict requirements, including
      risk-mitigation systems, high-quality data sets, clear user information,
      and human oversight.</span></li>
  <li class=MsoNormal style='margin-bottom:6.0pt;line-height:normal'><b><span
      lang=EN-US style='font-family:"Calibri",sans-serif'>Unacceptable Risk</span></b><span
      lang=EN-US style='font-family:"Calibri",sans-serif'>: AI systems that
      pose a clear threat to people’s fundamental rights, such as social
      scoring by governments, are banned.</span></li>
 </ul>
 <li class=MsoNormal style='margin-bottom:6.0pt;line-height:normal'><b><span
     lang=EN-US style='font-family:"Calibri",sans-serif'>Compliance</span></b><span
     lang=EN-US style='font-family:"Calibri",sans-serif'>: Ensuring that AI
     systems, including generative AI and LLMs, meet the requirements set out
     by the AI Act and other relevant regulations. This includes conducting
     conformity assessments, maintaining technical documentation, and
     registering high-risk AI systems in the EU database.</span></li>
 <li class=MsoNormal style='margin-bottom:6.0pt;line-height:normal'><b><span
     lang=EN-US style='font-family:"Calibri",sans-serif'>Human Oversight</span></b><span
     lang=EN-US style='font-family:"Calibri",sans-serif'>: Maintaining human
     oversight in AI decision-making processes to ensure accountability and
     transparency. This includes having human operators who can intervene in AI
     operations, providing clear instructions for users, and ensuring that AI
     decisions can be explained and justified.</span></li>
</ul>

<p class=MsoNormal style='line-height:107%'><b><span lang=EN-US
style='font-family:"Calibri",sans-serif'>Example</span></b><span lang=EN-US
style='font-family:"Calibri",sans-serif'>: Imagine an AI system used to analyze
medical images for diagnosing diseases, powered by a generative AI model. Since
this is a high-risk application, it must comply with the AI Act’s strict
requirements. This includes using high-quality data sets to train the model,
implementing risk-mitigation systems to handle potential errors, and ensuring
that human doctors can review and override the AI’s decisions. Additionally,
the system must be registered in the EU database, and clear information must be
provided to users about how the AI works and its limitations. This ensures that
the AI system operates safely, ethically, and transparently, protecting patient
rights and maintaining trust in healthcare AI.</span></p>

<p class=MsoNormal style='line-height:107%'><span lang=EN-US style='font-family:
"Calibri",sans-serif'>Generative AI and large language models (LLMs), such as
GPT-3 and GPT-4, used in conversational agents for patient interaction, must
comply with specific transparency requirements. For instance, they must clearly
inform users that they are interacting with a machine, ensuring transparency.</span></p>

<span lang=EN-US style='font-size:12.0pt;line-height:115%;font-family:"Calibri",sans-serif'><br
clear=all style='page-break-before:always'>
</span>

<p class=MsoNormal><span lang=EN-US style='font-size:20.0pt;line-height:115%;
font-family:"Calibri",sans-serif;color:#0F4761'>&nbsp;</span></p>

</div>

<span lang=EN-US style='font-size:12.0pt;line-height:115%;font-family:"Aptos",sans-serif'><br
clear=all style='page-break-before:always'>
</span>

<div class=WordSection2>

<p class=MsoNormal><a name="_Chapter_5:_Explainable"></a><span lang=EN-US>&nbsp;</span></p>

</div>

</body>

</html>
